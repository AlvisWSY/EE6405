{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pineapple/opt/anaconda3/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-23 20:38:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 28.7MB/s]                    \n",
      "2024-08-23 20:38:55 INFO: Downloaded file to /Users/pineapple/stanza_resources/resources.json\n",
      "2024-08-23 20:38:55 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-23 20:38:56 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| mwt          | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "======================================\n",
      "\n",
      "2024-08-23 20:38:56 INFO: Using device: cpu\n",
      "2024-08-23 20:38:56 INFO: Loading: tokenize\n",
      "2024-08-23 20:38:56 INFO: Loading: mwt\n",
      "2024-08-23 20:38:56 INFO: Loading: pos\n",
      "2024-08-23 20:38:57 INFO: Loading: constituency\n",
      "2024-08-23 20:38:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ROOT                         \n",
      "          |                            \n",
      "          S                           \n",
      "  ________|_________                   \n",
      " |                  VP                \n",
      " |    ______________|_________         \n",
      " |   |         |              PP      \n",
      " |   |         |           ___|____    \n",
      " NP  |         NP         |        NP \n",
      " |   |     ____|____      |        |   \n",
      "PRP VBD   DT        NN    IN       NN \n",
      " |   |    |         |     |        |   \n",
      "She took the      lesson  to     heart\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')\n",
    "\n",
    "text = 'She took the lesson to heart'\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "parse_tree = nltk.Tree.fromstring(str(doc.sentences[0].constituency))\n",
    "\n",
    "parse_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           ROOT                               \n",
      "                            |                                  \n",
      "                            S                                 \n",
      "    ________________________|_________                         \n",
      "   |                                  VP                      \n",
      "   |       ___________________________|_____________           \n",
      "   |      |                 NP                      |         \n",
      "   |      |       __________|_____                  |          \n",
      "   |      |      |                PP                PP        \n",
      "   |      |      |     ___________|___          ____|___       \n",
      "   NP     |      NP   |               NP       |        NP    \n",
      "   |      |      |    |      _________|___     |     ___|___   \n",
      "  NNS    VBD    NNS   IN    NN        CC  NN   IN   DT      NN\n",
      "   |      |      |    |     |         |   |    |    |       |  \n",
      "workers dumped sacks  of garbage     and junk into  a      bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"workers dumped sacks of garbage and junk into a bin\"\n",
    "\n",
    "# 使用 stanza 解析文本\n",
    "doc = nlp(text)\n",
    "\n",
    "# 获取句法树并转换为 nltk 的树状结构\n",
    "parse_tree = nltk.Tree.fromstring(str(doc.sentences[0].constituency))\n",
    "\n",
    "# 打印句法树\n",
    "parse_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers (nsubj) <-- dumped\n",
      "dumped (ROOT) <-- dumped\n",
      "sacks (dobj) <-- dumped\n",
      "of (prep) <-- sacks\n",
      "garbage (pobj) <-- of\n",
      "and (cc) <-- garbage\n",
      "junk (conj) <-- garbage\n",
      "into (prep) <-- dumped\n",
      "a (det) <-- bin\n",
      "bin (pobj) <-- into\n",
      "        dumped                  \n",
      "    ______|__________________    \n",
      "   |            sacks        |  \n",
      "   |              |          |   \n",
      "   |              of        into\n",
      "   |              |          |   \n",
      "   |           garbage      bin \n",
      "   |       _______|_____     |   \n",
      "workers  and           junk  a  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import CFG\n",
    "from nltk import Tree\n",
    "\n",
    "# 加载模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"workers dumped sacks of garbage and junk into a bin\"\n",
    "\n",
    "# 解析文本\n",
    "doc = nlp(text)\n",
    "\n",
    "# 打印依存关系\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ({token.dep_}) <-- {token.head.text}\")\n",
    "\n",
    "# 定义一个递归函数，将spacy的依存关系转换为nltk的树状结构\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "# 获取句子的根节点\n",
    "root = [sent.root for sent in doc.sents][0]\n",
    "\n",
    "# 转换为nltk的树状结构\n",
    "nltk_tree = to_nltk_tree(root)\n",
    "\n",
    "# 打印树状结构\n",
    "nltk_tree.pretty_print()\n",
    "\n",
    "# 手动构建CFG\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> NNS | NNS PP\n",
    "  VP -> VBD NP | VBD NP PP\n",
    "  PP -> IN NP\n",
    "  NNS -> 'workers'\n",
    "  VBD -> 'dumped'\n",
    "  NP -> NNS | NNS PP | NN | NN CC NN | DT NN\n",
    "  NN -> 'sacks' | 'garbage' | 'junk' | 'bin'\n",
    "  IN -> 'of' | 'into'\n",
    "  DT -> 'a'\n",
    "  CC -> 'and'\n",
    "\"\"\")\n",
    "\n",
    "# 解析器\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# 输入句子\n",
    "sentence = \"workers dumped sacks of garbage and junk into a bin\".split()\n",
    "\n",
    "# 生成句法树\n",
    "for tree in parser.parse(sentence):\n",
    "    tree.pretty_print()\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
